\subchapter{Analiza algorytmu quicksort}

\exercise %7.4-1
Niech $c$, $n_0>0$ będą takimi stałymi, że dla wszystkich $n\ge n_0$ wyrażenie $cn$ ogranicza od dołu składnik $\Theta(n)$ ze wzoru na $T(n)$.
Ustalmy pewne $n\ge n_0$.
Zgadujemy, że dla wszystkich $q=0$, 1, \dots, $n-1$ i~pewnej stałej $d>0$ zachodzi $T(q)\ge dq^2$.
Podstawiamy do wzoru na $T(n)$ i~na podstawie wyniku \refExercise{7.4-3} otrzymujemy:
\begin{align*}
	T(n) &\ge \max_{0\le q\le n-1}(dq^2+d(n-q-1)^2)+cn \\
	&= d\cdot\!\!\!\max_{0\le q\le n-1}(q^2+(n-q-1)^2)+cn \\
	&= d(n-1)^2+cn \\
	&= dn^2+n(c-2d)+d \\
	&\ge dn^2.
\end{align*}
Ostatnia nierówność jest spełniona, o~ile składnik $n(c-2d)+d$ jest nieujemny, co można osiągnąć przez wybór odpowiednio małego $d$.

Przyjmijmy $T(0)=1$ za przypadek brzegowy rekurencji.
Dla dowolnego $d$ zachodzi $T(0)>d\cdot0^2=0$, a~więc $n=0$ przyjmujemy na podstawę indukcji.
To kończy dowód, że $T(n)=\Omega(n^2)$.

\exercise %7.4-2
Czas algorytmu quicksort w~przypadku optymistycznym jest opisany przez rekurencję
\[
    T(n) = \min_{0\le q\le n-1}(T(q)+T(n-q-1))+\Theta(n).
\]
Pokażemy, że $T(n)=\Omega(n\lg n)$, używając metody podstawiania.

Wpierw jednak wyznaczymy minimum wyrażenia $(q+1)\lg(q+1)+(n-q)\lg(n-q)$ (dla $0\le q\le n-1$), które pojawi się w~dowodzie oszacowania.
Potraktujmy je jak funkcję $f$ zmiennej $q$ i~wyznaczmy jej pierwszą i~drugą pochodną:
\begin{align*}
    f'(q) &= \frac{d}{dq}\biggl(\frac{(q+1)\ln(q+1)+(n-q)\ln(n-q)}{\ln2}\biggr) = \frac{\ln(q+1)-\ln(n-q)}{\ln2}, \\[1mm]
	f''(q) &= \frac{d}{dq}\biggl(\frac{\ln(q+1)-\ln(n-q)}{\ln2}\biggr) = \frac{1}{\ln2}\biggl(\frac{1}{q+1}+\frac{1}{n-q}\biggr) = \frac{n+1}{\ln2\cdot(q+1)(n-q)}.
\end{align*}
Pierwsza pochodna zeruje się tylko wtedy, gdy $q+1=n-q$, skąd $q=(n-1)/2$.
Wyznaczamy wartość drugiej pochodnej w~tym punkcie:
\[
    f''\biggl(\frac{n-1}{2}\biggr) = \frac{4(n+1)}{\ln2\cdot(n+1)(n+1)} = \frac{4}{\ln2\cdot(n+1)} > 0.
\]
W~związku z~powyższym stwierdzamy, że funkcja $f$ przyjmuje minimum w~punkcie $q=(n-1)/2$ wynoszące $(n+1)\lg((n+1)/2)$.

Podobnie jak w~poprzednim zadaniu, niech $c$, $n_0>0$ będą stałymi takimi, że $cn$ stanowi ograniczenie dolne składnika $\Theta(n)$ dla wszystkich $n\ge n_0$.
Niech $d>0$ będzie inną stałą i~niech $n\ge n_0$.
Zgadujemy, że $T(q)\ge d(q+1)\lg(q+1)$ dla każdego $q=0$, 1, \dots, $n-1$ i~stosujemy to oszacowanie dla $T(q)$ i~$T(n-q-1)$:
\begin{align*}
    T(n) &\ge \min_{0\le q\le n-1}(d(q+1)\lg(q+1)+d(n-q)\lg(n-q))+cn \\
    &\ge d(n+1)\lg\frac{n+1}{2}+cn \\
	&= d(n+1)\lg(n+1)-d(n+1)+cn \\
	&= d(n+1)\lg(n+1)+n(c-d)-d \\
	&\ge d(n+1)\lg(n+1).
\end{align*}
Ostatnia nierówność jest spełniona, o~ile dobierzemy wystarczająco małe $d$ tak, aby wyrażenie $n(c-d)-d$ było nieujemne.

Przyjmijmy, że przypadkiem brzegowym rekurencji jest $T(0)=1$.
Wówczas zachodzi $T(0)>d\cdot1\cdot\lg1=0$, dlatego $n=0$ można przyjąć jako podstawę indukcji.

\exercise %7.4-3
Potraktujmy wyrażenie jako funkcję $f(q)=q^2+(n-q-1)^2$, gdzie $0\le q\le n-1$.
W~celu znalezienia maksimum globalnego tej funkcji wyznaczmy jej pochodne:
\[
    f'(q) = 2q-2(n-q-1) \quad\text{oraz}\quad f''(q) = 4.
\]
Ponieważ druga pochodna jest dodatnia, to funkcja $f$ nie posiada maksimum lokalnego i~jej największej wartości należy szukać w~punktach brzegowych dziedziny.
Mamy $f(0)=f(n-1)=(n-1)^2$, więc maksimum jest osiągane w~obu tych punktach.

\exercise %7.4-4
Przy wyznaczaniu dolnego oszacowania na oczekiwany czas działania algorytmu quicksort korzystamy z~analizy przedstawionej w sekcji 7.4.2 dla górnego oszacowania.
Zauważmy, że lemat 7.1 pozostaje prawdziwy, gdyby zamiast notacji $O$ zastosować $\Omega$.
Prowadząc rozumowanie analogicznie, dochodzimy w~końcu do wartości oczekiwanej zmiennej losowej $X$, którą następnie ograniczamy od dołu, wykorzystując wzór (A.13), a~następnie (3.18):
\[
	\E(X) = \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac{2}{k+1} \ge \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac{2}{2k} \ge \sum_{i=1}^{n-1}\ln(n-i+1) = \sum_{i=2}^n\ln i = \frac{\lg(n!)}{\lg e} = \Omega(n\lg n).
\]

\exercise %7.4-5
W~rozważanej modyfikacji drzewo rekursji w~algorytmie quicksort ma około $\lg(n/k)$ poziomów, z~których każdy wnosi koszt $O(n)$.
W~przypadku średnim czas wykonania tego kroku wynosi $O(n\lg(n/k))$.
Liczba fragmentów o~mniej niż $k$ elementach, których nie uporządkowano w~pierwszej fazie, jest rzędu $O(n/k)$.
Drugi krok polega na posortowaniu ich przez wstawianie i~zajmuje czas $O(n/k\cdot k^2)=O(nk)$.
Stąd całkowitym oczekiwanym czasem działania algorytmu jest $O(nk+n\lg(n/k))$.

Teoretycznie parametr $k$ powinien być rzędu co najwyżej $O(\lg n)$ -- wtedy złożoność czasowa tego algorytmu nie przewyższa złożoności czasowej zwykłego quicksorta.
W~praktyce jednak $k$ powinno być dobrane eksperymentalnie tak, aby sortowanie przez wstawianie tablicy o~długości $k$ było wykonywane szybciej od sortowania takiej tablicy algorytmem quicksort.

\exercise %7.4-6
Niech $P$ będzie szukanym prawdopodobieństwem zależnym od parametru $0<\alpha<1$.
Zauważmy, że funkcja ta spełnia własność $P(\alpha)=P(1-\alpha)$, w~rozwiązaniu będziemy więc zakładać, że $\alpha\le1/2$.
Niech $i<j<k$ będą indeksami elementów losowo wybranych z~$A$.
Podział $\alpha$ do $(1-\alpha)$ powstaje w~najgorszym przypadku wtedy i~tylko wtedy, gdy $\alpha n\le j\le(1-\alpha)n$.
Rozważmy możliwe przypadki wyboru $i$ oraz $k$ i~obliczmy prawdopodobieństwa ich zajścia:
\[
	\begin{array}{rcrclcl}
	p_{00} &=& \Pr\bigl(i<\alpha n &\text{i}& (1-\alpha)n<k\bigr) &\approx& 6\alpha^2(1-2\alpha), \\[1mm]
	p_{01} &=& \Pr\bigl(i<\alpha n &\text{i}& \alpha n\le k\le(1-\alpha)n\bigr) &\approx& 3\alpha(1-2\alpha)^2, \\[1mm]
	p_{10} &=& \Pr\bigl(\alpha n\le i\le(1-\alpha)n &\text{i}& (1-\alpha)n<k\bigr) &\approx& 3\alpha(1-2\alpha)^2, \\[1mm]
	p_{11} &=& \Pr\bigl(\alpha n\le i\le(1-\alpha)n &\text{i}& \alpha n\le k\le(1-\alpha)n\bigr) &\approx& (1-2\alpha)^3.
	\end{array}
\]
Dostajemy w~końcu $P(\alpha)=p_{00}+p_{01}+p_{10}+p_{11}\approx4\alpha^3-6\alpha^2+1$.
