\chapter{Quicksort -- sortowanie szybkie}

\subchapter{Opis algorytmu}

\exercise %7.1-1
Rys.~\ref{fig:7.1-1} przedstawia działanie procedury \proc{Partition} dla tablicy~$A$.
\begin{figure}[ht]
	\begin{center}
		\includegraphics{fig07.1}
	\end{center}
	\caption{Symulacja działania procedury \proc{Partition}} \label{fig:7.1-1}
\end{figure}

\exercise %7.1-2
Zauważmy, że podczas testowania elementów podtablicy w~wierszu~4 procedury \proc{Partition}, warunek ten będzie spełniony dla wszystkich elementów podtablicy, kiedy wszystkie będą sobie równe. Oznacza to, że po wykonaniu pętli \kw{for} z~wierszy 3\nobreakdash--6, będzie $i=r-1$ i~na końcu swego działania procedura zwróci wynik równy $r$.

Odpowiedniej modyfikacji procedury można dokonać poprzez wprowadzenie licznika elementów równych elementowi rozdzielającemu $x$ w~badanej podtablicy. Wartość licznika byłaby na końcu procedury testowana i~jeśli byłaby ona równa $r-p+1$, czyli rozmiarowi podtablicy, zwracana byłaby wartość $(p+r)/2$.

\exercise %7.1-3
Podczas przetwarzania podtablicy $A[p\twodots r]$ o~rozmiarze $n=r-p+1$ wykonywanych jest $n-1$ przebiegów pętli \kw{for}, a~każdy z~nich składa się z~operacji wykonywanych w~stałym czasie, stąd czas działania procedury wynosi $\Theta(n)$.

\exercise %7.1-4
Wystarczy zamienić znak nierówności na przeciwny w~warunku z~wiersza~4 procedury \proc{Partition}.

\subchapter{Czas działania algorytmu quicksort}

\exercise %7.2-1
Korzystając z~założenia, że $T(n-1)\le c_1(n-1)^2$, dostajemy
\begin{align*}
	T(n) &= T(n-1)+\Theta(n) \\
	&\le c_1(n-1)^2+d_1n \\
	&= c_1n^2+(d_1-2c_1)n+1 \\
	&\le c_1n^2,
\end{align*}
ponieważ można dobrać takie stałe $c_1$, $d_1>0$, aby zachodziła ostatnia nierówność dla każdego $n\ge1$, np.\ $c_1=d_1=1$.

Dolne oszacowanie dowodzimy analogicznie, wychodząc z~założenia $T(n-1)\ge c_2(n-1)^2$ i~zamieniając znak nierówności na przeciwny w~powyższym dowodzie. Dla analogicznych stałych $c_2$, $d_2>0$ przyjmujemy wartościowanie $c_2=1/2$, $d_2=2$.

Przypadek brzegowy $T(1)=1$ można przyjąć za podstawę obu indukcji dla dobranych stałych, a~więc zostało uzasadnione, że $T(n)=\Theta(n^2)$.

\exercise %7.2-2
Procedura \proc{Partition} w~takim przypadku zwróci wartość $r$ (z~\zad{7.1-2}), a~więc w następnych wywołaniach rekurencyjnych będą przetwarzane podtablice o~rozmiarach, odpowiednio, $n-1$ i~0. Napotykając w~każdym wywołaniu rekurencyjnym na przypadek pesymistyczny, algorytm będzie działał w~czasie opisanym przez rekurencję z~\zad{7.2-1}, czyli $\Theta(n^2)$.

\exercise %7.2-3
W~tym przypadku elementem rozdzielającym $x$ w~procedurze \proc{Partition} będzie element najmniejszy w~przetwarzanej podtablicy $A[p\twodots r]$. Jedyna dokonana zamiana to wymiana elementu rozdzielającego z~$A[p]$ i wartością zwracaną jest $r$. Przypadek jest zatem analogiczny do rozważanego w~poprzednim zadaniu, a~więc czasem działania \proc{Quicksort} jest $\Theta(n^2)$.

\exercise %7.2-4
Prawie posortowany ciąg jest niemal najgorszym przypadkiem dla algorytmu quicksort, wykonywane są bowiem niezrównoważone podziały, przez co czas działania sortowania zbliża się do kwadratowego względem rozmiaru tablicy. Z~kolei dla sortowania przez wstawianie ciąg taki jest przypadkiem zbliżonym do optymistycznego, ponieważ czas działania procedury \proc{Insertion-Sort} jest tego samego rzędu, co ilość inwersji w~ciągu wejściowym, a ta dla ciągu prawie posortowanego jest na tyle mała, że sortowanie przez wstawianie może działać w~czasie liniowym.

\exercise %7.2-5
Rozważmy drzewo rekursji dla opisanego przypadku dokonywania podziałów. Najkrótsza ścieżka w~drzewie składa się z~węzłów o~wartościach $\alpha^in$, gdzie $i$ jest poziomem węzła, zaś najdłuższa ścieżka na \twoparts{$i$}{tym} poziomie posiada węzeł o~wartości $(1-\alpha)^in$. Niech $h$ będzie głębokością liścia najkrótszej krawędzi. Mamy wtedy $\alpha^hn=1$, skąd
\[
	h = \log_\alpha\frac{1}{n} = -\frac{\lg n}{\lg\alpha}.
\]
Analogicznie, niech $H$ stanowi głębokość liścia na gałęzi najdłuższej. Zachodzi $(1-\alpha)^Hn=1$, więc
\[
	H = \log_{(1-\alpha)}\frac{1}{n} = -\frac{\lg n}{\lg(1-\alpha)}.
\]

\exercise %7.2-6
Równoważnie, należy wykazać, że procedura \proc{Partition} utworzy podział mniej zrównoważony niż $1-\alpha$ do $\alpha$ z~prawdopodobieństwem około $2\alpha$. W takim przypadku węzły na najkrótszej gałęzi drzewa rekursji będą wnosić koszt $\alpha'^in\le\alpha^in$ na \twoparts{$i$}{tym} poziomie. Ponieważ $0<\alpha\le1/2$, to wykorzystując definicję prawdopodobieństwa o~rozkładzie jednostajnym, dostajemy
\[
	\Pr(\alpha'\le\alpha) = \frac{\alpha}{1/2} = 2\alpha,
\]
co należało udowodnić.

\subchapter{Randomizowana wersja algorytmu quicksort}

\exercise %7.3-1
Randomizacja algorytmu powoduje, że nie można dobrać danych wejściowych tak, aby stanowiły one przypadek pesymistyczny. Każdy egzemplarz danych wymusza działanie algorytmu dla przypadku średniego, nie ma więc sensu rozważanie czasu działania w~innych przypadkach.

\exercise %7.3-2
W~przypadku pesymistycznym generator liczb losowych może wybierać wartości, które tworzą najbardziej niezrównoważony podział, na przykład element najmniejszy lub największy w~przetwarzanej tablicy. W~takiej sytuacji liczba wywołań generatora jest rzędu $\Theta(n)$.

W~przypadku optymistycznym, gdy tworzone są podziały najbardziej zrównoważone, generator wybiera element rozdzielający mniej więcej w~połowie podtablicy $A[p\twodots r]$, a~więc liczba jego wywołań wynosi wtedy $\Theta(\lg n)$.

\subchapter{Analiza algorytmu quicksort}

\exercise %7.4-1
Zgadujemy, że $T(n)\ge dn^2$ dla pewnej stałej $d>0$ i~podstawiamy:
\begin{align*}
	T(n) &\ge \max_{0\le q\le n-1}(dq^2+d(n-q-1)^2)+\Theta(n) \\
	&= d\cdot\max_{0\le q\le n-1}(q^2+(n-q-1)^2)+\Theta(n) \\
	&= d(n-1)^2+\Theta(n) \\
	&= dn^2-(2dn-1)+\Theta(n) \\
	&\ge dn^2,
\end{align*}
przy czym ostatnia nierówność zostaje spełniona, jeśli przyjmiemy $d$ odpowiednio małe tak, aby składnik $\Theta(n)$ zdominował wyrażenie $2dn-1$. W~kroku bazowym indukcji mamy $T(1)=1\ge d$, a więc dla odpowiednio małej wartości $d$ warunek jest spełniony, co kończy dowód, że $T(n)=\Omega(n^2)$.

\exercise %7.4-2
% Niech $T(n)$ będzie czasem działania algorytmu quicksort w~przypadku optymistycznym. Wtedy
% \[
% 	T(n) = \min_{0\le q\le n-1}(T(q)+T(n-q-1))+\Theta(n).
% \]
% Zgadujemy, że rozwiązaniem powyższej rekurencji jest $T(n)\ge cn\lg n$ dla pewnej stałej $c>0$. Podstawiając oszacowanie dla $T(q)$ i~$T(n-q-1)$, dostajemy
% \begin{align*}
% 	T(n) &\ge \min_{0\le q\le n-1}(cq\lg q+c(n-q-1)\lg(n-q-1))+\Theta(n) \\
% 	&= c\cdot\min_{0\le q\le n-1}(q\lg q+(n-q-1)\lg(n-q-1))+\Theta(n).
% \end{align*}
% 
% Rozważmy funkcję $f(q)=q\lg q+(n-q-1)\lg(n-q-1)$ i~wyznaczmy jej minimum dla $0\le q\le n-1$. Obliczamy pierwszą pochodną:
% \begin{align*}
% 	\frac{df}{dq}(q) &= \frac{d}{dq}\left(\frac{q\ln q+(n-q-1)\ln(n-q-1)}{\ln2}\right) \\
% 	&= \frac{\ln q+1-\ln(n-q-1)-1}{\ln2} \\
% 	&= \frac{\ln q-\ln(n-q-1)}{\ln2}
% \end{align*}
% i~po przyrównaniu jej do~0, otrzymujemy $q=(n-1)/2$. Aby zweryfikować, że w~tym punkcie $f$ posiada minimum, obliczmy jej drugą pochodną:
% \[
% 	\frac{d^2\!f}{dq^2}(q) = \frac{1}{\ln2}\left(\frac{1}{q}-\frac{1}{n-q-1}\right),
% \]
% przy czym zachodzi
% \[
% 	\frac{d^2\!f}{dq^2}(\frac{n-1}{2}) = \frac{1}{\ln2}\left(\frac{2}{n-1}-\frac{2}{n-1}\right) = \frac{4}{(n-1)\ln2},
% \]
% co jest liczbą dodatnią, o~ile $n\ge2$, a~zatem minimum zostało potwierdzone.
% 
% Wracając do oszacowania rekurencji, mamy
% \begin{align*}
% 	T(n) &\ge c(n-1)\lg\frac{n-1}{2}+\Theta(n) \\
% 	&= c(n-1)\lg(n-1)-c(n-1)+\Theta(n) \\
% 	&= cn\lg(n-1)-c\lg(n-1)-c(n-1)+\Theta(n).
% \end{align*}
% Zakładając teraz, że $n\ge2$, mamy
% \begin{align*}
% 	T(n) &\ge cn\lg(n/2)-c\lg(n-1)-c(n-1)+\Theta(n) \\
% 	&= cn\lg n-(2cn+c\lg(n-1)-c)+\Theta(n) \\
% 	&\ge cn\lg n,
% \end{align*}
% ponieważ dobierając stałą $c$ odpowiednio małą, spełniamy ostatnią nierówność, bo składnik $\Theta(n)$ dominuje nad wyrażeniem $2cn+c\lg(n-1)-c$. Przyjęcie $T(1)=1$ za podstawę indukcji oczywiście spełnia oszacowanie, bo $T(1)\ge0$, a~zatem pokazaliśmy, że $T(n)=\Omega(n\lg n)$.

Do wyniku można dojść analogicznie jak do górnego oszacowania na czas działania algorytmu w~przypadku pesymistycznym, rozważając w~rekurencji~(7.1) minimum wyrażenia $q^2+(n-q-1)^2$ zamiast jego maksimum. Jednak prościej jest zauważyć, że przypadek optymistyczny występuje wtedy, gdy dokonywane podziały tablicy są zawsze najbardziej zrównoważone, tzn.\ podział następuje mniej więcej na środku tablicy. Czas działania algorytmu sprowadza się zatem do rekurencji
\[
	T(n) = \begin{cases}
		\Theta(1), & \text{jeśli $n=1$,} \\
		2T(\lfloor n/2\rfloor)+\Theta(n), & \text{jeśli $n>1$,}
	\end{cases}
\]
której rozwiązaniem jest $\Omega(n\lg n)$ (z~\zad{4.1-2}).

\exercise %7.4-3
Potraktujmy wyrażenie jako funkcję $f(q)=q^2+(n-q-1)^2$, gdzie $0\le q\le n-1$. Szukamy jej maksimum globalnego, które może się znajdować w~punkcie, w~którym jej pierwsza pochodna zeruje się lub na krańcach dziedziny $f$. Pierwsza pochodna ma postać $df(q)/dq=2q-2(n-q-1)$ i~zeruje się w~punkcie $(n-1)/2$, a~więc obliczamy wartości:
\begin{align*}
	f\left(\frac{n-1}{2}\right) &= \frac{(n-1)^2}{2}, \\
	f(0) &= (n-1)^2, \\
	f(n-1) &= (n-1)^2.
\end{align*}
Widać, że funkcja przyjmuje maksimum dla $q=0$ lub $q=n-1$; trzecia obliczona wartość stanowi jej minimum.

\exercise %7.4-4
Przy wyznaczaniu dolnego oszacowania na oczekiwany czas działania algorytmu quicksort, korzystamy z~analizy przedstawionej w~tekście dla górnego oszacowania. Zauważmy, że lemat~7.1 pozostaje prawdziwy, gdyby zamiast notacji $O$ zastosować $\Omega$. Prowadząc rozumowanie analogicznie, dochodzimy w~końcu do wartości oczekiwanej zmiennej losowej $X$, którą następnie ograniczamy od dołu:
\[
	\E(X) = \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac{2}{k+1} \ge \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac{2}{2k} = \sum_{i=1}^{n-1}H_{n-i} = \sum_{i=1}^{n-1}H_i = \sum_{i=1}^{n-1}\Omega(\lg i) = \Omega(n\lg n).
\]
Skorzystaliśmy tutaj z~\zad{A.2-3} oraz z~problemu~A.1(b) dla $s=1$, dzięki czemu zachodzi ostatnia równość.

\exercise %7.4-5
Zauważmy, że liczba fragmentów mniej niż \twoparts{$k$}{elementowych} pozostawionych nieposortowanymi przez quicksort wynosi $O(n/k)$. Stąd czas wykonywany przez sortowanie przez wstawianie, który jest tego samego rzędu, co liczba inwersji pozostawionych w~całej tablicy, wynosi $O(n/k\cdot k^2)=O(nk)$. Czas wykonywania pozostałej części algorytmu obliczmy następująco. Jest to wywołanie oryginalnego quicksorta, który jednak zatrzymuje się na tablicy o~długości mniejszej niż $k$. Od czasu pełnego sortowania szybkiego należy zatem odjąć czas działania wszystkich wywołań zastosowanych do tych małych podtablic. Mamy stąd $O(n\lg n-(n/k)k\lg k)=O(n\lg(n/k))$ i~całkowitym czasem działania algorytmu jest $O(nk+n\lg(n/k))$.

Teoretycznie, parametr $k$ powinien być rzędu co najwyżej $O(\lg n)$ -- wtedy czas działania tego algorytmu nie przewyższa czasu działania sortowania szybkiego. W~praktyce jednak $k$ powinno być tak dobrane, aby sortowanie przez wstawianie tablicy o~długości $k$ było rzeczywiście szybsze od sortowania algorytmem quicksort.

\exercise %7.4-6
Załóżmy dla uproszczenia, że $n$ jest dowolnie duże, przez co możemy pozwolić sobie na pewną niedbałość przy wyznaczaniu indeksów tablicy. Oznaczmy fragmenty tablicy $A$: $X=A[1\twodots\alpha n]$, $Y=A[\alpha n\twodots(1-\alpha)n]$, $Z=A[(1-\alpha)n\twodots n]$ i~zauważmy, że aby mediana z~trzech elementów $a$, $b$,~$c$ losowo wybranych z~$A[1\twodots n]$ należała do $Y$, możliwe jest zajście następujących przypadków:
\begin{itemize}
	\item $a\in X$, $b\in Y$, $c\in Z$, co zachodzi z~prawdopodobieństwem $6\alpha^2(1-2\alpha)$,
	\item $a\in X$, $b\in Y$, $c\in Y$, co zachodzi z~prawdopodobieństwem $3\alpha(1-2\alpha)^2$,
	\item $a\in Y$, $b\in Y$, $c\in Z$, co zachodzi z~prawdopodobieństwem $3\alpha(1-2\alpha)^2$,
	\item $a\in Y$, $b\in Y$, $c\in Y$, co zachodzi z~prawdopodobieństwem $(1-2\alpha)^3$.
\end{itemize}
Sumując cząstkowe prawdopodobieństwa, otrzymujemy wynik $4\alpha^3-6\alpha^2+1$.

\problems

\problem{Poprawność algorytmu podziału Hoare'a} %7-1

\subproblem %7-1(a)
Symulacja procedury \proc{Hoare-Partition} dla przykładowej tablicy $A$ została przedstawiona na rys.~\ref{fig:7-1a}. Pierwsza część rysunku przestawia wejściową tablicę i~początkowe ustawienie zmiennych, kolejne części obrazują stan tablicy i~zmienne po każdej iteracji pętli \kw{while}.
\begin{figure}[ht]
	\begin{center}
		\includegraphics{fig07.2}
	\end{center}
	\caption{Symulacja działania procedury \proc{Hoare-Partition}} \label{fig:7-1a}
\end{figure}

\subproblem %7-1(b)

\subproblem %7-1(c)
Prawdziwość stwierdzenia wynika z~faktu udowodnionego w punkcie~(b) oraz stąd, że procedura kończy działanie dopiero, gdy $i>j$. Zauważmy, że musi nastąpić przynajmniej jedna zamiana elementów z~linii~10, co powoduje, że po tej zamianie $j<r$. Skoro $i\le r$, to musi zachodzić $p\le j<r$.

\subproblem %7-1(d)
Wynika to z~warunków stopu obu pętli \kw{repeat} i~testu z~wiersza~9. Każda para elementów $\langle A[i],A[j]\rangle$, gdzie $i<j$ oraz $A[i]\ge x$ i~$A[j]\le x$, jest odwracana; sprawia to, że elementy, które naruszały warunek, po zamianie znajdują się w~odpowiednich częściach tablicy. W~momencie, gdy indeksy $i$ oraz $j$ miną się, wszystkie elementy w~podtablicy $A[p\twodots j]$ są mniejsze od każdego elementu z~$A[j+1\twodots r]$.

\subproblem %7-1(e)
Jedyną różnicą w~porównaniu z~algorytmem quicksort jest to, że element rozdzielający nie znajduje się w~$A[q]$ po wykonaniu \proc{Hoare-Partition}, musimy więc sortować rekurencyjnie fragment $A[p\twodots q]$ zamiast $A[p\twodots q-1]$. Pseudokod procedury został przedstawiony poniżej.
\begin{codebox}
\Procname{$\proc{Hoare-Quicksort}(A,p,r)$}
\li	\If $p<r$
\li		\Then
			$q\gets\proc{Hoare-Partition}(A,p,r)$
\li			$\proc{Hoare-Quicksort}(A,p,q)$
\li			$\proc{Hoare-Quicksort}(A,q+1,r)$
		\End
\end{codebox}

\problem{Alternatywna analiza algorytmu quicksort} %7-2

\subproblem %7-2(a)
W~randomizowanej wersji procedury \proc{Partition} element $A[r]$ jest zamieniany z~losowo wybranym spośród całej tablicy $A$. Stąd, jeśli ma ona rozmiar $n$, to szanse, że pewien jej element zostanie elementem rozdzielającym, wynoszą $1/n$.

Można teraz zdefiniować zmienną losową wskaźnikową $X_i=\I($\twoparts{$i$}{ty} najmniejszy element jest wybrany jako rozdzielający$)$, dla której $\E(X_i)=\Pr(X_i=1)=1/n$ gdzie $i=1$, 2,~\dots,~$n$.

\subproblem %7-2(b)
Wywołanie \proc{Randomized-Quicksort} z~tablicą wejściową $A$ o~długości $n$ potrzebuje $\Theta(n)$ czasu na podział tablicy. Zostaje wyznaczony indeks $q$, a~następnie jest wywoływana rekurencyjnie \proc{Randomized-Quicksort} z~podtablicami wielkości $q-1$ i~$n-q$. Stąd oczekiwany czas potrzebny na posortowanie \twoparts{$n$}{elementowej} tablicy można wyrazić równaniem
\[
	\E(T(n)) = \E\biggl(\sum_{q=1}^nX_q(T(q-1)+T(n-q)+\Theta(n))\biggr),
\]
gdzie $X_q$ jest zmienną losową zdefiniowaną w~części~(a).

\subproblem %7-2(c)
Wykorzystując część~(a) i~(b) oraz liniowość wartości oczekiwanej, otrzymujemy
\begin{align*}
	\E(T(n)) &= \sum_{q=1}^n\E\bigl(X_q(T(q-1)+T(n-q)+\Theta(n))\bigr) \\
	&= \sum_{q=1}^n\E(X_q)\bigl(\E(T(q-1))+\E(T(n-q))+\E(\Theta(n))\bigr) \\
	&= \sum_{q=1}^n\frac{1}{n}\biggl(\E(T(q-1))+\E(T(n-q))+\Theta(n)\biggr) \\
	&= \frac{1}{n}\sum_{q=0}^{n-1}2\E(T(q))+\frac{1}{n}\sum_{q=1}^n\Theta(n) \\
	&= \frac{2}{n}\sum_{q=0}^{n-1}\E(T(q))+\Theta(n)
\end{align*}

\subproblem %7-2(d)
Wprowadzamy podział sumy na dwie części:
\[
	\sum_{k=1}^{n-1}k\lg k = \sum_{k=1}^{\lceil n/2\rceil-1}k\lg k+\sum_{k=\lceil n/2\rceil}^{n-1}k\lg k,
\]
po czym zauważamy, że funkcja $\lg k$ w~pierwszej sumie po prawej stronie jest ograniczona z~góry przez $\lg(n/2)=\lg n-1$, a~funkcja $\lg k$ w~drugiej sumie -- przez $\lg n$. Stąd
\begin{align*}
	\sum_{k=1}^{n-1}k\lg k &\le (\lg n-1)\sum_{k=1}^{\lceil n/2\rceil-1}k+\lg n\sum_{k=\lceil n/2\rceil}^{n-1}k, \\
	&= \lg n\sum_{k=1}^{n-1}k-\sum_{k=1}^{\lceil n/2\rceil-1}k \\[2mm]
	&\le \frac{n(n-1)\lg n}{2}-\frac{n}{4}\left(\frac{n}{2}-1\right) \\[1mm]
	&\le \frac{n^2\lg n}{2}-\frac{n^2}{8}.
\end{align*}

\subproblem %7-2(e)
Korzystając ze wskazówki, przyjmijmy założenie indukcyjne $\E(T(k))\le ak\lg k-bk$ dla $k=1$, 2,~\dots~$n-1$ oraz pewnych stałych $a$, $b>0$. Mamy
\begin{align*}
	\E(T(n)) &= \frac{2}{n}\sum_{k=1}^{n-1}\E(T(k))+\Theta(n) \\
	&\le \frac{2}{n}\sum_{k=1}^{n-1}(ak\lg k-bk)+\Theta(n) \\
	&= \frac{2a}{n}\sum_{k=1}^{n-1}k\lg k-b(n-1)+\Theta(n).
\end{align*}
Wykorzystując teraz wynik z~części~(d), dostajemy
\begin{align*}
	\E(T(n)) &\le \frac{2a}{n}\left(\frac{n^2\lg n}{2}-\frac{n^2}{8}\right)-b(n-1)+\Theta(n) \\
	&\le an\lg n-\frac{an}{4}-bn+b+\Theta(n) \\
	&\le an\lg n-bn,
\end{align*}
ponieważ możemy wybrać $a$ dość duże, żeby wartość wyrażenia $an/4$ dominowała nad wartością wyrażenia $\Theta(n)+b$. Stąd widać, że średni czas działania algorytmu quicksort wynosi $O(n\lg n)$.

\problem{Nieefektywne sortowanie} %7-3

\subproblem %7-3(a)
Udowodnimy poprawność algorytmu przez indukcję względem rozmiaru tablicy $n$. Łatwo sprawdzić, że algorytm poprawnie sortuje w~przypadku, gdy $n\le2$. Załóżmy więc, że $n>2$ i~że poprawnie sortowane są tablice o~długościach mniejszych niż $n$. Wtedy po wykonaniu wiersza~7, $k$ największych elementów $A[i\twodots j]$ znajduje się we fragmencie $A[j-k-1\twodots k]$. Następnie, w~wierszu~8, sortowany jest fragment $A[i\twodots j-k]$, dzięki czemu cała tablica $A[i\twodots j]$ zostaje poprawnie posortowana.

\subproblem %7-3(b)
Procedura jest wywoływana rekurencyjnie 3~razy, zaś każde wywołanie otrzymuje tablicę o~rozmiarze około $2/3$ rozmiaru oryginalnej tablicy. Ponadto praca poza wywołaniami rekurencyjnymi jest wykonywana w~czasie stałym. Stąd dostajemy równanie rekurencyjne
\[
	T(n) =
	\begin{cases}
		\Theta(1), & \text{jeśli $n\le2$}, \\
		3T(2n/3)+\Theta(1), & \text{jeśli $n>2$},
	\end{cases}
\]
które rozwiązujemy przy użyciu twierdzenia o~rekurencji uniwersalnej, otrzymując wynik $T(n)=O(n^{\log_{3/2}3})\approx O(n^{2{.}71})$.

\subproblem %7-3(c)
Czas działania algorytmu \proc{Stooge-Sort} jest, jak widać, wyższego rzędu nie tylko od algorytmów sortowania przez scalanie, kopcowanie czy sortowania szybkiego, ale również od mniej efektywnego sortowania przez wstawianie. Metoda sortowania profesorów jest więc bezużyteczna.

\problem{Głębokość stosu w~algorytmie quicksort} %7-4

\subproblem %7-4(a)
\proc{Quicksort'} działa w~taki sam sposób jak oryginalny algorytm, z~tą tylko różnicą, że drugie wywołanie rekurencyjne, które sortuje prawą podtablicę, zostaje zasymulowane w~pętli bez wykonywania rzeczywistego wywołania. Poprawność algorytmu wynika zatem z~poprawności oryginalnego algorytmu quicksort.

\subproblem %7-4(b)
Stos rekurencji może urosnąć do rozmiaru $\Theta(n)$ w~sytuacji, gdy tablica o~$n$ elementach będzie dzielona za każdym razem na podtablice, z~których pierwsza posiada $n-1$ elementów, a~druga jest tablicą pustą. Na każdym poziomie rekursji procedura \proc{Partition} zwraca wtedy wartość $r$. Takie zachowanie algorytmu powoduje tablica posortowana rosnąco.

\subproblem %7-4(c)
Wykorzystamy pomysł, aby wywoływać procedurę rekurencyjnie dla mniejszej podtablicy, natomiast większą przetwarzać w~bieżącym wywołaniu. Po wywołaniu \proc{Partition}, wystarczy testować, która z części tablicy jest mniejsza i~tę część przekazywać w~wywołaniu rekurencyjnym. Oczekiwany czas działania algorytmu nie zmieni się, natomiast największą głębokością stosu rekurencji będzie teraz $\Theta(\lg n)$, co zachodzi w~przypadku, gdy tablica będzie za każdym razem dzielona mniej więcej w~połowie.

\problem{Podział względem mediany trzech wartości} %7-5

\subproblem %7-5(a)
Załóżmy, że element, który będzie medianą, znajduje się na pozycji $i$ w~tablicy $A'$. Musimy więc wybrać jeszcze jeden element na lewo od $i$, czyli jeden z~$i-1$ oraz jeden leżący na prawo od $i$, czyli jeden spośród $n-i$ elementów. Otrzymujemy zatem
\[
	p_i = \frac{(i-1)(n-i)}{\binom{n}{3}}.
\]

\subproblem %7-5(b)
W~oryginalnej strategii prawdopodobieństwo wyboru dowolnego elementu z~tablicy $A[1\twodots n]$ na element rozdzielający wynosi $1/n$ (z~części~(a) problemu 7.2). Z~kolei przy wyborze mediany z~trzech mamy
\[
	p_{\lfloor(n+1)/2\rfloor} \approx \frac{(n/2)(n/2)}{\binom{n}{3}} = \frac{3n}{2(n-1)(n-2)}.
\]
Na przybliżenie przy obliczaniu powyższej wartości możemy sobie pozwolić, gdyż niedokładność zostanie zniwelowana przez fakt, że $n\to\infty$. Stosunek tych prawdopodobieństw wynosi
\[
	\lim_{n\to\infty}\frac{p_{\lfloor(n+1)/2\rfloor}}{1/n} = \lim_{n\to\infty}\frac{3n^2}{2(n-1)(n-2)} = \frac{3}{2},
\]
czyli szanse na to, że mediana tablicy $A[1\twodots n]$ zostanie wybrana na element rozdzielający, zwiększą się o~50\% przy dowolnie dużym $n$.

\subproblem %7-5(c)
W~tradycyjnym podejściu szansa na uzyskanie dobrego podziału wynosi
\[
	\sum_{i=n/3}^{2n/3}\frac{1}{n} = \frac{1}{3}+\frac{1}{n} \approx \frac{1}{3}.
\]
Obliczmy teraz szanse na dobry podział przy wyborze mediany z~trzech:
\[
	\sum_{i=n/3}^{2n/3}p_i = \sum_{i=n/3}^{2n/3}\frac{(i-1)(n-i)}{\binom{n}{3}} = \frac{6}{n(n-1)(n-2)}\underbrace{\sum_{i=n/3}^{2n/3}(i-1)(n-i)}_{S(n)},
\]
przy czym ostatnią sumę można przybliżyć następująco:
\[
	S(n) \approx \sum_{i=n/3}^{2n/3}i(n-i) \approx 2\sum_{i=n/3}^{n/2}i(n-i),
\]
co następnie przybliżamy za pomocą całki:
\begin{align*}
	S(n) &\approx 2\int_{n/3}^{n/2}x(n-x)\,dx = 2x^2\left(\frac{n}{2}-\frac{x}{3}\right)\Big|_{n/3}^{n/2} \\
	&= 2\left(\frac{n}{2}\right)^2\left(\frac{n}{2}-\frac{n}{6}\right)-2\left(\frac{n}{3}\right)^2\left(\frac{n}{3}-\frac{n}{9}\right) = \frac{13n^3}{6\cdot27}.
\end{align*}
Powracamy teraz do oszacowania szans dobrego podziału, otrzymując
\[
	\sum_{i=n/3}^{2n/3}p_i = \frac{6S(n)}{n(n-1)(n-2)} \approx \frac{6}{n(n-1)(n-2)}\cdot\frac{13n^3}{6\cdot27} = \frac{13n^2}{27(n-1)(n-2)}.
\]

Wzrost szans po zastosowaniu nowej strategii wyboru elementu rozdzielającego wyznaczymy, obliczając stosunek prawdopodobieństw, który wynosi
\[
	\frac{13n^2}{9(n-1)(n-2)},
\]
co przy $n\to\infty$ jest bliskie $13/9$, wzrost wynosi więc prawie 50\%.

\subproblem %7-5(d)
Nowy sposób wyboru elementu rozdzielającego zwiększa jedynie szanse na uzyskanie podziału zrównoważonego, co z~kolei drastycznie obniża prawdopodobieństwo, że algorytm quicksort będzie działał w~czasie kwadratowym. Jednakże dolne oszacowanie na czas działania algorytmu pozostaje niezmienione i~wynosi nadal $\Omega(n\lg n)$ -- można sobie wyobrazić sytuację, w~której oryginalny sposób wyboru elementu rozdzielającego wprowadza za każdym razem najbardziej zrównoważony podział.

\problem{Rozmyte sortowanie przedziałów} %7-6

\subproblem %7-6(a)
Zauważmy na wstępie, że jeśli dla przedziałów $[a_1,b_1]$ i~$[a_2,b_2]$ zachodzi warunek $a_1\le a_2\le b_1$ albo $a_2\le a_1\le b_2$, to można je ustawić w~dowolnej kolejności w~wynikowej tablicy. Zaprojektujemy algorytm, który działa podobnie jak quicksort sortujący po lewych krańcach przedziałów. Po wyborze elementu rozdzielającego $x=[a_0,b_0]$ sprawdzamy kolejne przedziały i~umieszczamy je w~różnych częściach tablicy -- podczas sprawdzania przedziału $i_j=[a_j,b_j]$, jeśli $a_j>a_0$, to przedział $i_j$ zostanie umieszczony na prawo od $x$. W~przeciwnym przypadku sprawdzamy jeszcze, czy $b_j>a_0$, a~więc czy przedziały $x$ oraz $i_j$ nachodzą na siebie. W~zależności od wyniku tego testu, $i_j$ umieszczamy w~innej podtablicy. Wyznaczone przedziały nachodzące na $x$ możemy, według wcześniej zauważonego warunku, uporządkować w~dowolnej kolejności; podtablica do której one należą nie wymaga zatem dalszego sortowania. Natomiast pozostałe podtablice sortowane są rekursywnie tak jak w~algorytmie quicksort.

Przedstawione rozumowanie zapiszmy w~postaci pseudokodu:
\begin{codebox}
\Procname{$\proc{Fuzzy-Quicksort}(A,p,r)$}
\end{codebox}

\subproblem %7-6(b)
Oczekiwany czas $\Theta(n\lg n)$ wynika z~oczekiwanego czasu działania sortowania szybkiego -- może się zdarzyć, że podtablica zawierająca przedziały nachodzące na $x$ będzie za każdym razem pusta.

\endinput
