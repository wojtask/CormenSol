\subchapter{Kody Huffmana}

\exercise %16.3-1
Drzewo $T$, które nie jest regularne, zawiera co najmniej jeden węzeł $x$ o~stopniu 1.
Jeśli węzeł ten zostanie ,,wycięty'' z~$T$, to głębokość każdego potomka węzła $x$ zmniejszy się o~1 i~powstałe w~ten sposób drzewo będzie mieć koszt niższy od kosztu $T$.
Można go dalej obniżać przez pozbywanie się w~ten sposób wszystkich węzłów stopnia 1.

\exercise %16.3-2
Jeden z~optymalnych kodów Huffmana dla podanego zbioru liter i~ich częstości ilustruje rys.\ \ref{fig:16.3-2}.
\begin{figure}[!ht]
	\centering \input{fig16.3-2}
	\caption{{\sffamily\bfseries(a)} Optymalny kod Huffmana dla liter \texttt{a}, \texttt{b}, \dots, \texttt{h} o~częstościach będących początkowymi liczbami Fibonacciego.
{\sffamily\bfseries(b)} Drzewo odpowiadające temu kodowi uzyskane w~wyniku działania procedury \proc{Huffman}.} \label{fig:16.3-2}
\end{figure}

Zastanówmy się, jak może wyglądać optymalny kod Huffmana dla $n$ liter o~częstościach będących początkowymi liczbami Fibonacciego.
Przyjrzyjmy się wartości \attrib{z}{f} obliczanej w~algorytmie \proc{Huffman} w~linii 7.
Nietrudno zauważyć, że w~$i$\nbhyphen tej iteracji pętli \kw{for} jest to suma najmniejszych $i+1$ częstości liter, czyli suma $i+1$ początkowych liczb Fibonacciego:
\[
	\attrib{z}{f} = \sum_{k=1}^{i+1}F_k = F_{i+3}-1.
\]
Gdy $i>1$, to $F_{i+2}<\attrib{z}{f}<F_{i+3}$, dlatego \attrib{z}{f} w~iteracji $i+1$ jest drugą najmniejszą wartością w~kolejce $Q$ i~węzeł o~tej wartości zostaje wybrany na prawego syna kolejnego węzła $z$.
W~pierwszej iteracji zachodzi $\attrib{z}{f}=2=F_3$.
Jeśli kolejka priorytetowa zaimplementowana jest jako kopiec binarny typu min na podstawie informacji w~rozdziale 6, to także w~tym przypadku doprowadzi to do opisanego powiązania węzłów.
Zbudowane drzewo będzie mieć wysokość $n-1$, przy czym lewy syn każdego jego węzła wewnętrznego będzie liściem.
W~otrzymanym kodzie Huffmana, literze o~częstości $F_k$, gdzie $3\le k\le n$, przypisane zostanie słowo kodowe postaci
\[
	\underbrace{11\cdots1}_{n-k}0.
\]
Litery o~częstościach $F_1=F_2=1$ są nierozróżnialne z~punktu widzenia tego problemu, więc następujące słowa kodowe
\[
	\underbrace{11\cdots1}_{n-2}0 \quad\text{oraz}\quad \underbrace{11\cdots1}_{n-1}
\]
mogą być przypisane do tych liter w~dowolnej kolejności.

\exercise %16.3-3
Dowód przeprowadzimy przez indukcję względem wysokości drzewa $h$.
Jeśli $h=0$, czyli drzewo $T$ składa się z~jednego węzła, to $B(T)=0$ i~twierdzenie trywialnie zachodzi.

Niech teraz $h>0$.
Przez $I(T)$ oznaczymy zbiór węzłów wewnętrznych drzewa $T$, a~przez $L(T)$ -- zbiór jego liści.
Załóżmy, że twierdzenie zachodzi dla lewego poddrzewa $T_L$ i~prawego poddrzewa $T_R$ drzewa $T$, czyli
\[
	B(T_L) = \sum_{x\in I(T_L)}\bigl(\attribb{x}{left}{f}+\attribb{x}{right}{f}\bigr) \quad\text{oraz}\quad B(T_R) = \sum_{x\in I(T_R)}\bigl(\attribb{x}{left}{f}+\attribb{x}{right}{f}\bigr).
\]
Częstość każdego węzła jest równa sumie częstości obu jego synów, a~te z~kolei są sumami częstości swoich synów itd.
Stąd częstość węzła można przedstawić jako sumę po wszystkich liściach znajdujących się w~poddrzewie o~korzeniu w~tym węźle, w~szczególności $\attribb{T}{root}{f}=\sum_{x\in L(T)}\attrib{x}{f}$.
Mamy więc:
\begin{align*}
	B(T) &= \sum_{x\in L(T)}\attrib{x}{f}d_T(x) \\[1mm]
	&= \sum_{x\in L(T_L)}\attrib{x}{f}d_T(x)+\sum_{x\in L(T_R)}\attrib{x}{f}d_T(x) \\[1mm]
	&= \sum_{x\in L(T_L)}\attrib{x}{f}(d_{T_L}(x)+1)+\sum_{x\in L(T_R)}\attrib{x}{f}(d_{T_R}(x)+1) \\[1mm]
	&= \sum_{x\in L(T_L)}\attrib{x}{f}d_{T_L}(x)+\sum_{x\in L(T_R)}\attrib{x}{f}d_{T_R}(x)+\sum_{x\in L(T)}\attrib{x}{f} \\[1mm]
	&= B(T_L)+B(T_R)+\attribb{T}{root}{f} \\[1mm]
	&= \sum_{x\in I(T_L)}\bigl(\attribb{x}{left}{f}+\attribb{x}{right}{f}\bigr)+\sum_{x\in I(T_R)}\bigl(\attribb{x}{left}{f}+\attribb{x}{right}{f}\bigr) \\
	& \phantom{{}=B(T_L)+B(T_R)}+\attribbb{T}{root}{left}{f}+\attribbb{T}{root}{right}{f} \\[1mm]
	&= \sum_{x\in I(T)}\bigl(\attribb{x}{left}{f}+\attribb{x}{right}{f}\bigr).
\end{align*}

\exercise %16.3-4
Niech $C=\{c_1,c_2,\dots,c_n\}$ będzie zbiorem znaków o~częstościach $f(c_1)\ge f(c_2)\ge\dots\ge f(c_n)$.
Niech $T$ będzie drzewem odpowiadającym optymalnemu kodowi Huffmana dla znaków ze zbioru $C$.
Przypomnijmy, że głębokość liścia reprezentującego znak $c\in C$ w~drzewie $T$ jest długością słowa kodowego przypisanego do znaku $c$.
Udowodnimy żądaną własność nie-wprost, poprzez założenie, że istnieją takie $1\le i<j\le n$, że $d_T(c_i)>d_T(c_j)$ i~doprowadzenie do sprzeczności.

Suma kosztów wnoszonych przez znaki $c_i$ i~$c_j$ do łącznego kosztu drzewa $B(T)$ wynosi $f(c_i)d_T(c_i)+f(c_j)d_T(c_j)$.
Porównajmy ją z~sumą kosztów tych znaków w~sytuacji, gdy są one zamienione miejscami w~drzewie $T$.
Różnica
\[
	\bigl(f(c_i)d_T(c_i)+f(c_j)d_T(c_j)\bigr)-\bigl(f(c_i)d_T(c_j)+f(c_j)d_T(c_i)\bigr) = \bigl(f(c_i)-f(c_j)\bigr)\bigl(d_T(c_i)-d_T(c_j)\bigr)
\]
jest dodatnia, na podstawie przyjętych założeń.
A~zatem drzewo powstałe z~$T$ poprzez zamianę miejscami znaków $c_i$ i~$c_j$ ma niższy koszt niż drzewo $T$, dlatego $T$ nie może reprezentować optymalnego kodowania.
Wynika stąd, że w~optymalnym drzewie każda para znaków o~niemalejących częstościach zajmuje nierosnące głębokości, a~więc słowa kodowe tej pary znaków także mają nierosnące długości.

\exercise %16.3-5
Na podstawie \refExercise{16.3-1} drzewo $T$ reprezentujące optymalne kodowanie zbioru $C$ jest regularne, więc posiada $n$ liści i~$n-1$ węzłów wewnętrznych.
Jego strukturę możemy zakodować, przechodząc je w~porządku preorder i~zamiast wypisywać klucz węzła, odnotowywać 0, jeśli jest to węzeł wewnętrzny, albo 1, jeśli jest on liściem.
Ponieważ drzewo $T$ jest regularne, to opisane kodowanie jest jednoznaczne i~składa się z~$2n-1$ bitów.
Należy jeszcze uzupełnić je o~informacje, któremu liściowi w~$T$ odpowiada który znak z~$C$.
Zauważmy, że do zapisania każdego znaku z~$C$ w~postaci binarnej, przy pomocy zwykłego kodu o~stałej długości, potrzeba $\lceil\lg n\rceil$ bitów.
Można więc użyć łącznie $n\lceil\lg n\rceil$ bitów do zapisania ciągu wszystkich znaków w~kolejności wyznaczonej przez liście w~kodowaniu drzewa $T$.

\exercise %16.3-6
Podobnie jak kody binarne składają się z~bitów, o~kodach trójkowych będziemy mówić, że składają się z~\textbf{tritów} przyjmujących wartości 0, 1 lub 2.
W~naszym algorytmie budowane będzie drzewo trójkowe odpowiadające optymalnemu trójkowemu kodu Huffmana.
Każdy węzeł $x$ takiego drzewa, oprócz pól \attrib{x}{left} i~\attrib{x}{right} wyposażony jest dodatkowo w~pole \attrib{x}{middle}, będące wskaźnikiem na środkowego syna $x$.

\begin{codebox}
\Procname{$\proc{Ternary-Huffman}(C)$}
\li	$n\gets|C|$
\li	\If $n\bmod2=0$
\li		\Then $C\gets C\cup\{\#\}$ \>\>\>\>\Comment \# -- specjalny znak o~częstości 0
		\End
\li	$Q\gets C$
\li	\For $i\gets1$ \To $\lfloor n/2\rfloor$ \label{li:ternary-huffman-for-begin}
\li		\Do utwórz nowy węzeł $w$
\li			$\attrib{w}{left}\gets x\gets\proc{Extract-Min}(Q)$
\li			$\attrib{w}{middle}\gets y\gets\proc{Extract-Min}(Q)$
\li			$\attrib{w}{right}\gets z\gets\proc{Extract-Min}(Q)$
\li			$\attrib{w}{f}\gets\attrib{x}{f}+\attrib{y}{f}+\attrib{z}{f}$
\li			$\proc{Insert}(Q,w)$
		\End \label{li:ternary-huffman-for-end}
\li	\Return $\proc{Extract-Min}(Q)$
\end{codebox}
Aby możliwe było zbudowanie drzewa regularnego, liczba znaków w~zbiorze $C$ musi być nieparzysta.
Jeśli jest inaczej, to alfabet $C$ zostaje rozszerzony o~dodatkowy specjalny znak~\#, dla którego przyjmujemy $\attrib{\#}{f}=0$.
Dodanie nowego znaku nie zmienia kosztu tego drzewa, zaś podczas odczytywania wynikowego kodowania znak ten może zostać bezpiecznie zignorowany.

Algorytm wykonuje $\lfloor n/2\rfloor$ iteracji pętli \kw{for} w~wierszach \ref{li:ternary-huffman-for-begin}\nbendash\ref{li:ternary-huffman-for-end} tworzących kolejne węzły wewnętrzne budowanego drzewa.
Z~kolejki priorytetowej $Q$ pobierane są trzy węzły $x$, $y$ i~$z$ o~najmniejszych częstościach, które stają się odpowiednio, lewym, środkowym i~prawym synem nowo utworzonego węzła $w$, a~za częstość węzła $w$ przyjmowana jest suma częstości węzłów $x$, $y$ i~$z$.
W~drzewie trójkowym zbudowanym w~algorytmie przyjmujemy, że krawędź do lewego syna jest etykietowana przez 0, krawędź do środkowego syna -- przez 1, a~krawędź do prawego syna -- przez 2.
Wówczas słowem kodowym znaku $c\in C$ jest ciąg tritów wyznaczonych przez etykiety na ścieżce od korzenia drzewa do liścia reprezentującego znak $c$.

Poprawność algorytmu \proc{Ternary-Huffman} wynika z~następujących lematów, które są wersjami lematów 16.2 i~16.3 rozszerzonymi na kody trójkowe.
Nie podajemy ich dowodów, ponieważ są one naturalnymi rozszerzeniami dowodów lematów dla kodów binarnych.

\bigskip
\noindent\textsf{\textbf{Lemat 16.2\/$'$.}} \textit{Niech\/ $C$ będzie alfabetem, w~którym częstością każdego znaku\/ $c\in C$ jest\/ \attrib{c}{f}.
Niech\/ $x$,\/ $y$ i\/~$z$ będą trójką znaków z\/~$C$ o~najmniejszych częstościach.
Istnieje wtedy optymalny kod prefiksowy dla\/ $C$, w~którym kody dla\/ $x$,\/ $y$ i\/~$z$ mają tę samą długość i~różnią się tylko ostatnim tritem.}

\bigskip
\noindent\textsf{\textbf{Lemat 16.3\/$'$.}} \textit{Niech\/ $C$ będzie alfabetem ze znakami\/ $c\in C$ o~częstościach\/ \attrib{c}{f}.
Niech\/ $x$,\/ $y$ i\/~$z$ będą trzema znakami z\/~$C$ o~najmniejszych częstościach.
Niech\/ $C'$ będzie alfabetem powstałym z\/~$C$ w~wyniku zastąpienia znaków\/ $x$,\/ $y$ i\/~$z$ znakiem\/ $w$ o~częstości równej sumie częstości\/ $x$,\/ $y$ i\/~$z$; tzn.\/\ $C'=C\setminus\{x,y,z\}\cup\{w\}$, częstość\/ $f$ każdego znaku w\/~$C'$ z~wyjątkiem\/ $w$ jest taka sama jak w\/~$C$, natomiast\/ $\attrib{w}{f}=\attrib{x}{f}+\attrib{y}{f}+\attrib{z}{f}$.
Niech\/ $T'$ będzie dowolnym drzewem reprezentującym pewny optymalny kod prefiksowy dla\/ $C'$.
Wówczas drzewo\/ $T$, otrzymane z\/~$T'$ przez zastąpienie liścia odpowiadającego\/ $w$ przez węzeł wewnętrzny z~trzema synami\/ $x$,\/ $y$ i\/~$z$, reprezentuje pewien optymalny kod prefiksowy dla alfabetu\/ $C$.}

\exercise %16.3-7
\note{Maksymalna częstość wystąpienia znaku powinna być ostro mniejsza niż dwukrotność najmniejszej częstości wystąpienia innego znaku.}

\noindent Jeśli w~zbiorze znaków najmniejszą częstością jest $a$, to największa częstość jest mniejsza niż $2a$.
W~algorytmie \proc{Huffman} każdy tworzony węzeł $z$ będzie miał częstość $\attrib{z}{f}\ge2a$, zatem każdy znak zostanie przetworzony i~usunięty z~kolejki $Q$, zanim zostanie pobrany z~niej węzeł $z$.
To sprawia, że każdy znak zajmie ten sam, najniższy poziom w~tworzonym drzewie, a~co za tym idzie, każdemu znakowi odpowiadać będzie słowo kodowe o~tej samej długości.
Nie jest to więc żadne usprawnienie w~porównaniu z~zastosowaniem kodu binarnego o~stałej długości, w~którym każdy znak zapisywany jest jako ciąg 8 bitów.

\exercise %16.3-8
Istnieje $2^{8k}$ możliwych plików składających się z~$k$ znaków 8\nbhyphen bitowych, czyli
\[
	C_n = \sum_{k=0}^n2^{8k} = \sum_{k=0}^n256^k = \frac{256^{n+1}-1}{255}
\]
plików zawierających co najwyżej $n$ znaków.
Jakakolwiek metoda kompresji każdemu takiemu plikowi przypisuje jednoznaczny plik skompresowany, dlatego potencjalna liczba plików skompresowanych wynosi co najmniej $C_n$.
Znaki w~pliku wejściowym pojawiają się losowo, toteż nic nie zyskamy przez przypisywanie krótszych plików skompresowanych jednym plikom wejściowym, a~dłuższych innym.
Statystycznie rzecz biorąc, nie da się więc zaoszczędzić nawet jednego bitu, stosując jakąkolwiek metodę kompresji dla losowych danych.
